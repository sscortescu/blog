---
title: "#28 I Die, Therefore I Am"
date: 2025-02-18
draft: false
--- 

Since I've started on SSRIs, there aren't many things that bring the cortisol-spike knot-in-my-chest back. This tweet did. Hello anxiety, old friend. 
![tweet](/intelligence/tweet.png#center)


# Cost of intelligence goes poof

The cost of intelligence is on a path to becoming negligible. Large Language Models are improving so fast that it's hard to even understand (graph and rate of change).  If you're using LLMs daily, maybe you can relate to this experience. The felt sense of using AI now is magical - I didn't get any invitation to Hogwards, but this is a close second. Advanced voice mode is NGL mind-blowing. That I can call ChatGPT mid-way through cooking a julienne to ask how to substitute wheat flour (with a French accent, please, thank you) is baffling. 

Julienne is the first recipe I learned from my grand mother. This Christmas I installed ChatGPT on her phone: Hey Grandma, check this out, you can even ask it to remind you take your medication, are you calling me senile, I'm only 80! She's probably just worried AI is going replace her (jk, she's retired, and irreplaceable, it's me who's worried). Maybe she cares more about spending time with her grandson than talking to robots. She definitely doesn't believe AI can make a better julienne than her (not yet, grandma, not yet). 

Grandma solemnly joins the choir of finger-raising-skeptics. I can hear them yelling "There are so many types of intelligence where AI is still so dumb!". Or "The trend will break - we're running out of training data. Even if we're not, performance may not scale with data infinitely!". Or "AI is so energy hungry that it will soon hit a capacity wall!" (Yes, grandma's new favorite words are "capacity wall")

Of course, emotional intelligence and empathy, spatial intelligence and dexterity, probably many other skills feel so deeply human that it's hard to imagine machines becoming great at them. But there are already impressive advances in AI understanding human emotion and expression. AI-made emotional expression is [not far behind either](https://www.youtube.com/watch?v=pWTTzR_wXuQ). Robots can now wash dishes, fold laundry ([almost a year ago already](https://www.youtube.com/watch?v=Sq1QZB5baNw)) - easy for us, notoriously difficult for them. 

I'm pretty sure they'll soon be able to smile more credibly than my Eastern-European face. 

What about data? There's only so much data, right? Right? Bring in the synthetic data! Not quite grandma-julienne-quality yet, but already being fed to the latest generation of models. Still hungry? Want that good old organic data? Enter brain-computer interfaces (BCI). While humans are currently limited by their brain's processing speed, what will happen when we connect the brain to...the Matrix? Neuralink and others are already parading paralyzed patients controlling computers with their thoughts (output). And they're working on restoring vision to blind patients by [connecting to the optic nerve](https://www.reuters.com/business/healthcare-pharmaceuticals/musks-neuralink-receives-fdas-breakthrough-device-tag-brain-implant-2024-09-17/) (input).

# SciFi goes brrr

But it's not just intelligence cost that is going to zero. Looking further into the future, with nuclear (both fusion and fission) seeing a renaissance (((LINK))), it's not *entirely* crazy to see the cost of energy going to zero too. Compute is following too. Google's new quantum chip ([Willow](https://blog.google/technology/research/google-willow-quantum-chip/)) spent 5 minutes solving a problem that takes the latest generation of supercomputers a septillion years (death-of-universe type of timescale, Rick and Morty type of vibes). 

I realize how crazy this sounds. From the above technologies, brain-computer interfaces are the most nascent. Maybe it's not even feasible to have a two-way broadband omnichannel (thoughts, senses, etc.) connection between human and machine. But smarter AI will accelerate research, into compute, energy, BCI, and yes, AI itself. The cost of resources that now seem prohibitive might soon become "just expensive" and not too far in the future "cheap".

New paradigms like increasing test-time compute (giving AI more time to think) are emerging. OpenAI's new reasoning models (o1 and now o3) are using this new approach in 2025 to blast past benchmarks ([source for image below](https://arcprize.org/blog/oai-o3-pub-breakthrough)) that seemed unachievable in 2024. The ARC challenge is based on puzzles that are easy for humans but hard for AI. GPT4o scored 9%, o3 scored 88% on ARC. Sure, it cost o3 >$1m to get to that score, but DeepSeeks's R1 [just wiped out 1 trillion](https://fortune.com/2025/01/27/deepseek-buzz-puts-tech-stocks-on-track-for-1-trillion-wipeout/) in stock market value (biggest wipe in history), largely because of their innovation that shrunk both training and inference costs massively. 
![arc](/intelligence/arc.png#center)
# So what?

Our value systems are evolving. Individual and Social. 

I only realized that "Intelligence" is one of my values about a year ago. I didn't like it, it felt shallow. 

The realization came after reading [Flowers for Algernon](https://www.goodreads.com/book/show/18373.Flowers_for_Algernon)  guy gets super smart from a science experiment, then watches himself lose it all), along with two others:
1. Intelligence as a value is "gifted" to us by society, because it's a proxy to power
2. Strip it away and there are still a lot of things that makes us - us. Humans - human. 

But being smart is a big part of my identity - yours too, if you "work on the computer". In fact, I try to become smarter, all the time. I have a yearly reading goal and I take nootropics. When interviewing for McKinsey (and for my probation period there), my toilet breaks were all about mental math training and brain games. I even sometimes worry about random brain farts - am I'm losing my edge.

Part of my sensitivity to intelligence is rooted in insecurity. Another part comes from intelligence being a tool I use for my daily meaning-making (i.e. work). Its cost going to zero is turning sensitivity into existential anxiety.

# Now what?

I can't help thinking of [Dan Simmon's Hyperion novels](https://www.goodreads.com/series/40461-hyperion-cantos). In Hyperion, humans live in a space age, where the AIs facilitate interstellar travel and communication while managing the economic system. Plugging into the matrix is thing, and humans can access any external knowledge through thought. 

But, the AIs hiddenly use the humans' brains for processing power. Because of course they do. 

Assuming that we end up building our own god (it's looking that way to me), and assuming that god is benevolent, we might end up living in something like a better Hyperion. The AIs provide sustenance, almost taking over the economy to cover human needs up and down the Maslow pyramid. In exchange, humans provide endless training data (their thoughts and experiences). 

Symbiosis, not parasitism. Some might say utopia. One of the best case scenarios, is what I'm thinking. Getting there is going to be a wild ride, though. 

Ignoring the military, political and economic tensions du jour, and only looking at what AI can already do, we can expect around 15% of Europe's jobs to be [impacted](https://www.perplexity.ai/search/read-a-news-article-saying-ai-ioqaIvmaQwuszAgRfK6xBg) this decade. What about the next five decades? What can we do to position ourselves, as individuals? Having a plan for your life used to be feasible 60-70 years ago. You could pick (or be assigned) a career, and execute your way to retirement. As things started moving faster, about 20-30 years ago, it became OK to change plans mid-way. The cost of intelligence dropping so fast will soon make "career plans" 2000-and-late. 

What can we hold on to?

My intuition tells me it will have something to do with what will stay unique to humans, no matter how capable AI becomes. 

# What makes us human?

We die. AI doesn’t. 

Death is something no one can really hold on to. But it's the origin of what makes us human. 
Mortality is the ultimate accountability mechanism. It injects urgency into beauty. AI doesn’t rush to finish a novel, panic about deadlines, or whisper _carpe diem_ to its robot friends. Death means everything we do is constrained by time. Time means trade-offs. Trade-offs mean stakes. Stakes mean risk. And risk changes the entire game.

We make decisions with imperfect information because we're terrible at calculating risk. We don’t even have a solid framework for it. Our brains are notoriously bad at understanding probability. Our models are simplistic, and half of the variables that matter are things we don’t even know exist ("Unknown unknowns," if you will). 

Despite that, we start businesses with 90% failure rates. We go for the kiss knowing rejection could hollow us out. We _voluntarily_ ride motorcycles. Experience teaches us the meaning of risk. It builds our tolerance, maybe even our appetite for it. 

AI may end up great at quantifying risk. But it will never be able to take any. 

In relationships, risk is tightly connected to trust. In turn, trust is built through credibility, reliability and intimacy. It's broken by by self-interest. AI might max out on reliability, but real vulnerability means having something to lose. AI may have no self-interest, which seems good. But if someone has _zero_ self-interest, we find them unnerving.

Speaking of relationships, when was the last time you felt seen or heard? Like, really? Most of us believe the *self* or the *consciousness* exists inside our heads, most of the time. That belief transfers to other people and things. As long as AI exists in the cloud, AI paying us attention - being with us, being present - is something we can't really buy. Even remote presence—when someone is watching _you_, giving _you_ their attention—has a weight that AI can’t replicate. 

One very human side of relationships are rituals. Birthdays, weddings, ~~initiations into adulthood~~ daily stand-ups. In turn, rituals are a reflection of our spirituality. Sacredness is our rebellion against meaninglessness and entropy. AI might eventually tap into something we'd call universal consciousness. It won't approach the sacred from our uniquely mortal perspective - with mystery and surrender. AI will know *everything*, and just as we don't believe it can give us attention, we won't believe it can *really* surrender.  

What can we hold on to? Our mortality, our ability to bear risks, our trust, attention, and spirituality. Our physicality, initiative and lateral thinking didn't make the list - I'm betting AI will catch up on these soon. 
# What can you do? 

When a resource becomes abundant, scarcity shifts elsewhere. Think electricity: once rare, now cheap. What became precious? Devices that _use_ electricity. "[Complements](https://en.wikipedia.org/wiki/Complementary_good)".

Slashing the cost of knowledge work outputs shifts the value to managing the work of the AI (e.g. copilots). We become the "complements" of AI for a while. We're moving up layers of abstraction, as long as we can keep up. 

Some of us will manage to [preserve their premium or even increase it](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321). Perhaps you can even skip layers entirely by mastering "with-AI" jobs that before were just emerging — think of a product manager who now also codes with AI.

To put it another way, the rules of competition didn't change. There are three main strategies: cost, focus, or differentiation. Trying to win on cost or focus against AI is betting against the curve. The only real play is differentiation. 

The silver lining is that entrepreneurs are the ultimate risk takers. They can be *the* AI complement, with AI actually helping to mitigate some of the risks. But the obvious ideas—efficient, scalable, data-driven—are the first to get swallowed whole. What’s left are the messy, human, and unpredictable opportunities. 

Everybody knows a good strategy has three phases, so here are three "horizons" to consider: 

**Short-term (<5 years):**
- **Become an AI Whisperer**: If AI is replacing knowledge work, become the person who knows how to wield it better than anyone else. Learn how to chain AI agents together, design workflows, and create systems that let one person outperform an entire team.

**Mid-term (5-10 years):**
- **Own Physical Scarcity:** Control access to things that AI can’t copy—small-batch artisanal goods, highly skilled manual labor , rare earth minerals, or physical space.
- **Bet on AI Skepticism:** People will pay to opt out of the AI economy. Sell human-first, AI-free goods, spaces, services, and experiences. The trend is already emerging with “dumb phones” and “human-curated” media.
- **Become a “Liability Holder”**:  Insert yourself into domains where mistakes have human consequences: doctors, law enforcement/military.
- **Sell undivided attention**: Provide therapy, coaching, or even just listening services. 

**Long-term (>10 years):**
- **YOLO like your life depends on it**: Ephemeral experiences, performance arts. Take it up a notch: Legacy architects, end-of-life doulas, grief retreats, chefs who curate “last meal” experiences.
------

As intelligence commoditizes, we’re handed a paradox: the less special thinking becomes, the more sacred living feels. AI will eclipse us in calculation, but it can’t replicate the stakes of existing—the heart-in-throat thrill of risking failure, the quiet awe of watching a sunset you’ll never see again. Grandma’s julienne recipe won’t be disrupted by an AI. It’ll be disrupted by her great-granddaughter selling it as a $10,000 “human heritage experience”… with a side of existential dread.